{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3 - Generative AI üñºÔ∏è\n",
    "\n",
    "### Intro üìù\n",
    "\n",
    "I denne hands-on-session vil all prosessering skje on-prem, uten √• koble til eksterne API-er. Dette er valgt for √• demonstrere hvordan konfidensielle data kan h√•ndteres trygt.\n",
    "\n",
    "OpenAI's teknologier, som er state-of-the-art, gir h√∏y ytelse men kommer ogs√• med usikkerheter. De er ikke transparente i hvordan data prosesseres, og det er en risiko for model drift, hvor modellens ytelse kan variere over tid. I tillegg kan APIer bli utdatert eller fjernet, noe som kan p√•virke systemets stabilitet.\n",
    "\n",
    "Denne notebook'en innholder ikke mangen oppgaver, s√• bruk gjerne *litt tid* p√• √• se hvordan ting er gjort og ekspreminter med √• endre parametere osv.\n",
    "\n",
    "Denne notebook'en er blitt inspirert av kurset [Generative AI AI av DeepLearning.ai](https://www.deeplearning.ai/courses/generative-ai-with-llms/)\n",
    "\n",
    "Kos dykk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start üî•\n",
    "\n",
    "F√∏r vi begynner, m√• vi sikre at n√∏dvendige biblioteker er installert. Selv om de fleste sannsynligvis allerede er p√• plass, er det lurt √• dobbeltsjekke for √• unng√• komplikasjoner senere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "print(\"OK :thumbsup:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi laster ned et datasett med dialoger, [DIALOGSum](https://huggingface.co/datasets/knkarthick/dialogsum).\n",
    "\n",
    "DialogSum er et strot datasett for dialogoppsummering, som best√•r av 13 460 dialoger (pluss 100 tilbakeholdte data for tema-generering) med tilh√∏rende manuelt merkede oppsummeringer og temaer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 240]\n",
    "\n",
    "dash_line = 50*'[bold black]-'\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print(f\"[bold magneta] Example {index}\")\n",
    "    print(dash_line)\n",
    "    print(\"[bold green]  INPUT DIALOGUE:\")\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print(\"[bold blue] :woman: BASELINE HUMAN SUMMARY:\")\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Det finnes flere st√∏rrelser, vi skal bruke en \"liten\" en\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "# Kan ta ~2-4 minutter hvis modeller har ikke lastet ned tidligere\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is fun!\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('[bold] :robot: ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('[bold] :baby: DECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave 1: Tokens vs word ID lookup\n",
    "\n",
    "Skriv en setning der du b√∏yer verbet, feks \"This is fun!\" -> \"This is funnier!\". \n",
    "Sjekk den encoded sentence, ser du noen likheter med forrige tensor? \n",
    "\n",
    "Hvilken fordeler er det med denne approachen vs feks. word ID lookup? ({\"fun\": 1, \"funnier\": 2, osv.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# La oss teste modellen, vi velger to tekster og sjekker hvordan modellen oppsummere teksten\n",
    "## Pr√∏v gjerne √• endre index p√• hvilken dialog du henter ut ü§ì\n",
    "example_indices = [40, 240]\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f'[bold magneta] Example {index}')\n",
    "    print(dash_line)\n",
    "    print(f'[bold green] INPUT PROMPT:')\n",
    "    print(f'{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'[bold blue] :woman: BASELINE HUMAN SUMMARY:')\n",
    "    print(summary)\n",
    "    print(dash_line)\n",
    "    print(f'[bold red] :robot: MODEL GENERATION - WITHOUT PROMPT ENGINEERING:')\n",
    "    print(f\"[italic] {output}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave\n",
    "\n",
    "Hvordan utf√∏rte modellen oppgaven?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promt-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shooting\n",
    "\n",
    "N√• skal vi gi litt mer instrukter til modellen hva den skal gj√∏re, men vi lar den svare p√• sp√∏rsm√•let alene.\n",
    "\n",
    "Vi lager en \"promt\" template og setter inn dialogen inn:\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "‚ÑπÔ∏è I Python kan du bruke f-strenger for √• formatere strenger enkelt. Du bruker et \"f\" foran strengen og setter variabler eller uttrykk i kr√∏llparenteser {}. Du kan legge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(f'[bold magneta] Example {index}')\n",
    "    print(dash_line)\n",
    "    print(f'[bold green] INPUT PROMPT:')\n",
    "    print(f'{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'[bold blue] :man: BASELINE HUMAN SUMMARY:')\n",
    "    print(summary)\n",
    "    print(dash_line)\n",
    "    print(f'[bold red] :robot: MODEL GENERATION - ZERO SHOT:')\n",
    "    print(f\"[italic] {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave\n",
    "Hvordan utf√∏rte modellen oppgaven?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot Learning\n",
    "\n",
    "N√• skal vise modellen hvordan en oppgave er utf√∏rt med sp√∏rsm√•let + svar, og dermed be den l√∏se neste oppgave ü§ì\n",
    "\n",
    "\n",
    "Her er v√•r promt denne gangen:\n",
    "```python\n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    \"\"\" Lag en promt basert p√• index \"\"\"\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        What was going on?\n",
    "        {summary}\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "            \n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        What was going on?\n",
    "        \"\"\"\n",
    "        \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Velger et eksempel med sp√∏rsm√•let+svaret\n",
    "example_indices_full = [40]\n",
    "\n",
    "# Hvilken tekst den skal oppsummere\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kj√∏rer modellen\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f'[bold magneta] Example {example_index_to_summarize}')\n",
    "print(dash_line)\n",
    "print(f'[bold blue] BASELINE HUMAN SUMMARY:')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print(f'[bold red] MODEL GENERATION - ONE SHOT:')\n",
    "print(f\"[italic] {output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shooting promt\n",
    "N√• gir vi modellen et lite antall eksempler, slik at den forst√•r mye mer av oppgaven den skal l√∏se ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Velger X eksempler med sp√∏rsm√•l og svar \n",
    "example_indices_full = [40, 80]\n",
    "\n",
    "# Velger oppgaven den skal l√∏se\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f'[bold magneta] Example {example_index_to_summarize}')\n",
    "print(dash_line)\n",
    "print(f'[bold blue] BASELINE HUMAN SUMMARY:')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print(f'[bold red] MODEL GENERATION - FEW SHOT:')\n",
    "print(f\"[italic] {output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave\n",
    "Pr√∏v √• legg inn flere eksempler og sjekk hva som skjer üòà\n",
    "\n",
    "Hva er ulempen √• legge inn mangen eksempler til modellen? Er det noen restriksjoner i selve modellen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave\n",
    "Test forskjellige parametere p√• √• generere tekst.\n",
    "\n",
    "Her er noen default parameters/values:\n",
    "```python\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=50, \n",
    "                                     do_sample=True, \n",
    "                                     temperature=1, \n",
    "                                     top_k=50,\n",
    "                                     repetition_penalty=1.0,\n",
    "                                     length_penalty=1,\n",
    "                                     top_p=1)\n",
    "```\n",
    "\n",
    "Sjekk i mer detaljer ang√•ende hvilken config som kan bli brukt [GenerationConfig](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig), se f. eks. p√• *Parameters for manipulation of the model output logits*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50, \n",
    "                                     do_sample=True, \n",
    "                                     temperature=1, \n",
    "                                     top_k=50,\n",
    "                                     repetition_penalty=1.0,\n",
    "                                     length_penalty=1,\n",
    "                                     top_p=1)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f'[bold magneta] Example {example_index_to_summarize}')\n",
    "print(dash_line)\n",
    "print(f'[bold blue] BASELINE HUMAN SUMMARY:')\n",
    "print(summary)\n",
    "print(dash_line)\n",
    "print(f'[bold red] MODEL GENERATION - FEW SHOT:')\n",
    "print(f\"[italic] {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Metric\n",
    "\n",
    "\n",
    "For √• verifisere kvaliteten p√• LLM-modellen sammenlignet med baseline, m√• vi bruke noen metrikker for √• m√•le tekstkvaliteten.\n",
    "\n",
    "To mye brukte metrikker innen NLP/LLM er [Rouge](https://en.wikipedia.org/wiki/ROUGE_(metric)) og [Bleu](https://en.wikipedia.org/wiki/BLEU). I dette tilfellet skal vi bare bruke Rouge.\n",
    "\n",
    "Vi vil teste standardmetrikker for √• m√•le [precision/recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) i LLM-oppgaver, og sammenligne strenger ved hjelp av [unigram og bi-gram](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/).\n",
    "\n",
    "Det finnes flere varianter, men vi skal kun se p√• de \"enkle\" ü§ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3'], use_stemmer=True)\n",
    "\n",
    "candidate_summary = \"it is cold outside, so get a jacket\"\n",
    "reference_summary = \"it is very cold outside, so get a warm jacket\"\n",
    "\n",
    "print(f\"{candidate_summary} <--> {reference_summary}\")\n",
    "print(dash_line)\n",
    "\n",
    "scores = scorer.score(reference_summary, candidate_summary)\n",
    "for key in scores:\n",
    "    print(f'[bold]{key}: {scores[key]}')\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "candidate_summary = \"Squatch eats pizza\"\n",
    "reference_summary = \"pizza eats Squatch\"\n",
    "\n",
    "print(f\"{candidate_summary} <--> {reference_summary}\")\n",
    "\n",
    "scores = scorer.score(reference_summary, candidate_summary)\n",
    "for key in scores:\n",
    "    print(f'[bold]{key}: {scores[key]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppgave\n",
    "\n",
    "Legg inn [rougeL](https://en.wikipedia.org/wiki/ROUGE_(metric)), hvordan l√∏ser den *Squatch eats pizza* oppgaven?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siste oppgave\n",
    "\n",
    "N√• skal vi m√•le hvordan Zero-Shot vs Few-Shot utf√∏rer oppgaven i storskala. For √• begynne litt sm√•tt, tester vi ut 10 datasett:\n",
    "```python\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "```\n",
    "\n",
    "Vi lagrer resultatet i en dataframe, slik at vi kan se p√• resultatet n√¶rmere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "zero_shot_summaries, few_shot_summaries = [], []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    zero_shot_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    zero_shot_text_output = tokenizer.decode(zero_shot_outputs[0], skip_special_tokens=True)\n",
    "    zero_shot_summaries.append(zero_shot_text_output)\n",
    "\n",
    "\n",
    "    # Vi velger to eksempler som skal inn i few-shot attempt v√•r\n",
    "    # Vi velger de to siste i datasettet.\n",
    "    example_indices_full = [-1]\n",
    "\n",
    "    # Velger oppgaven den skal l√∏se\n",
    "    few_shot_prompt = make_prompt(example_indices_full, idx)\n",
    "\n",
    "\n",
    "    input_ids = tokenizer(few_shot_prompt, return_tensors=\"pt\").input_ids\n",
    "    few_shot_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    few_shot_text_output = tokenizer.decode(few_shot_outputs[0], skip_special_tokens=True)\n",
    "    few_shot_summaries.append(few_shot_text_output)\n",
    "\n",
    "print(\"[bold red] DONE! :smiley:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_summaries = list(zip(human_baseline_summaries, zero_shot_summaries, few_shot_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'zero_shot_summaries', 'few_shot_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi bruker en fanzy Evaluate pakke fra hugginface\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "\n",
    "zero_shot_results = rouge.compute(\n",
    "    predictions=zero_shot_summaries,\n",
    "    references=human_baseline_summaries[0:len(zero_shot_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "few_shot_results = rouge.compute(\n",
    "    predictions=few_shot_summaries,\n",
    "    references=human_baseline_summaries[0:len(few_shot_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('[bold] Zero-shot:')\n",
    "for key, value in zero_shot_results.items():\n",
    "    print(f\"[bold magneta] {key}: {value:0.4f}\")\n",
    "\n",
    "print('[bold] Few-shot:')\n",
    "for key, value in few_shot_results.items():\n",
    "    print(f\"[bold magneta] {key}: {value:0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[bold] Absolutt prosentvis forbedring av few-shot-attempt over HUMAN BASELINE :chart:\")\n",
    "\n",
    "improvement = (np.array(list(few_shot_results.values())) - np.array(list(zero_shot_results.values())))\n",
    "for key, value in zip(few_shot_results.keys(), improvement):\n",
    "    print(f'[bold magneta] {key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oppgave\n",
    "\n",
    "Vi pr√∏vde egentlig **one-shot**, legg p√• noen flere eksempler i koden og se forbedringen üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ekstra oppgaver, hvis det er mer tid igjen\n",
    "\n",
    "* Test flere eksempler, √∏k til f. eks. 20 stk\n",
    "* Plot distribusjonen av en metrikk p√• zero/few-shot attempt. Hva ser du?\n",
    "* Kj√∏r en st√∏rre modell, se oversikten her [flan-t5 - huggingface](https://huggingface.co/docs/transformers/model_doc/flan-t5)\n",
    "    * ‚ö†Ô∏è **ADVARSEL** dette kommer til √• bruke mye minne(!) ü§Ø\n",
    "\n",
    "\n",
    "Det er mye dokumentasjon ang√•ende Flan-T5 modellene her p√• [huggingface](https://huggingface.co/docs/transformers/model_doc/t5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dagens Python Biblotek\n",
    "\n",
    "Hvis du kjeder deg √• skrive output/print meldinger, bruk [rich](https://github.com/Textualize/rich) biblioteket! Alt blir kjekkere med <span style=\"color:red\">emojis</span> og <span style=\"color:blue\">farger</span>. ü§ì üî•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
